---
title: GA - Google Merchandise Store Analysis
author: AndhikaWB
date: last-modified
date-format: long
format:
  gfm:
    output-file: README.md
knitr:
  opts_chunk:
    echo: true
    results: hold
    dev: svglite
    dev.args:
      scaling: 1.25
      bg: black
---

# Preparation

## Initial Setup

Make sure you already have [R](https://cran.r-project.org/bin/) installed, and also the R extension if you're using [R on VS Code](https://code.visualstudio.com/docs/languages/r)

Quarto markdown (.qmd) has better support than R markdown (.rmd) on VS Code. See the [get started](https://quarto.org/docs/get-started/), [formatting](https://quarto.org/docs/computations/r.html), and [execution](https://quarto.org/docs/computations/execution-options.html) docs for starter

Note that the .qmd file itself is just a template that needs to be knitted/compiled in order to produce the real .md file

## Overview

In [this competition](https://www.kaggle.com/competitions/ga-customer-revenue-prediction), youâ€™re challenged to analyze a Google Merchandise Store (also known as GStore, where Google swag is sold) customer dataset to predict revenue per customer. Hopefully, the outcome will be more actionable operational changes and a better use of marketing budgets for those companies who choose to use data analysis on top of GA data

## Library and Data Preparation

Load library
``` {r load-library}
#| warning: false

# Read from file (read_csv, etc)
library(readr)
# Data manipulation (mutate, group_by, etc)
library(dplyr)
# Data cleaning (unnest, etc)
library(tidyr)
# Data conditioning (map, keep, etc)
library(purrr)
# Parse/write JSON (parseJSON, etc)
library(jsonlite)
# See NA, N-unique, etc from data (skim)
library(skimr)
# Time manipulation (ymd, hour, etc)
library(lubridate)
# String manipulation (str_replace, etc)
library(stringr)

# Data visualization
library(ggplot2)
library(ggdark)
theme_set(dark_theme_gray())
# Easily combine multiple plot
library(patchwork)
# Word cloud for ggplot
library(ggwordcloud)
# Label auto scaling (percent, etc)
library(scales)

# Map data for ggplot
library(maps)
# Country name standardization
library(countrycode)
```

Load train and test data
``` {r load-data}
#| message: false
#| cache: true

# If it's too slow, use "n_max" argument to limit the rows
df_train <- read_csv('data/train.csv.zip')
df_test <- read_csv('data/test.csv.zip')

# Check if it's a Tibble
class(df_train)
```

Preview the raw data
``` {r preview-raw-data}
#| cache: true

cat('Train set dimension:', dim(df_train), '\n')
cat('Test set dimension:', dim(df_test), '\n')

glimpse(df_train)
```

## Data Preprocessing

Take a stratified sample from each day
``` {r take-sample}
#| cache: true

# For reproducibility
set.seed(1337)

# 0.01 = 1% of data
df_train <- df_train %>%
  group_by(date) %>%
  slice_sample(prop = 0.01) %>%
  ungroup

df_test <- df_test %>%
  group_by(date) %>%
  slice_sample(prop = 0.01) %>%
  ungroup

cat('Train set dimension (after sampling):', dim(df_train), '\n')
cat('Test set dimension (after sampling):', dim(df_test), '\n')
```

Wrap both data into a list
``` {r wrap-data}
# Wrap both data into a list for easier modification (e.g. using loop)
# We can't do this earlier due to list size limitation
dfl <- list(train = df_train, test = df_test)
rm(df_train, df_test)
```

Fix data types
``` {r fix-date-columns}
# We use [[]] if we want to access column/element name using string
# So instead of "dfl$train$date" we can use "dfl[['train']]$date"

# Loop both train and test data
for (df in names(dfl)) {
  # Fix date recognized as double
  dfl[[df]]$date <- as.Date.character(dfl[[df]]$date, format = '%Y%m%d')
  # Convert timestamp to datetime
  dfl[[df]]$visitStartTime <- as.POSIXct(dfl[[df]]$visitStartTime)
}

glimpse(dfl$train)
```

Un-nest JSON columns as separate columns
``` {r fix-json-columns}
#| warning: false

parseJSON <- function(df, col_name) {
  # Parse column as true JSON object
  # The comma and brackets are necessary for a valid JSON
  paste(df[[col_name]], collapse = ',') %>%
    paste('[', ., ']') %>% fromJSON
}

for (df in names(dfl)) {
  for (col in c('device', 'geoNetwork', 'totals', 'trafficSource')) {
    dfl[[df]] <- dfl[[df]] %>%
      # Without !! and := the mutate column name will always be "col"
      mutate(!!col := parseJSON(.data, col)) %>%
      unnest(cols = c(col), names_sep = '.')
  }
}

# trafficSource also has other nested children (adwordsClickInfo)
# This will cause error later so we need to un-nest them all
unnest_all <- function(df) {
  cols <- keep(df, is.data.frame) %>% colnames
  if (length(cols) == 0) return(df)
  # Un-nest again if still nested
  df <- df %>% unnest(cols = cols, names_sep = '.')
  return(unnest_all(df))
}

for (df in names(dfl)) {
  dfl[[df]] <- dfl[[df]] %>% unnest_all
}

glimpse(dfl$train)
```

Standardize various NA values
``` {r standardize-na}
na_vals <- c(
  'unknown.unknown', '(not set)', '(not provided)',
  'not available in demo dataset', '(none)', '<NA>', ''
)

for (df in names(dfl)) {
  for (col in colnames(dfl[[df]])) {
    # Note: sapply may change the column class type (e.g. date to double)
    # Hence we only check string columns to avoid unecessary conversion
    if (is.character(dfl[[df]][[col]])) {
      dfl[[df]][[col]] <- dfl[[df]][[col]] %>%
        sapply(., function(x) {
          # Replace all NA variants with standard NA
          replace(x, x %in% na_vals, NA)
        })
    }
  }
}

glimpse(dfl$train)
```

See data summary (unique values, NA, etc)
``` {r skim-summary}
skim(dfl$train) %>%
  # Without "focus", all details will be shown
  focus(
    n_missing,
    # Note that N-unique doesn't include NA
    character.n_unique,
    logical.count,
    # Median (percentile 50)
    numeric.p50,
    numeric.mean,
    Date.min,
    Date.max
  ) %>%
  print()
```

Remove columns that only have 1 possible value and show the rest
``` {r rm-non-unique-columns}
for (col in colnames(dfl$train)) {
  if (length(unique(dfl$train[[col]])) == 1) {
    dfl$train[[col]] <- NULL
    dfl$test[[col]] <- NULL
  }
}

cat('Number of rows:', dim(dfl$train)[1], '\n')

# Show possible values except NA
# If length < number of rows then the rest are NA
dfl$train %>%
  as.list %>%
  rapply(na.omit, how = 'replace') %>%
  str(give.attr = FALSE)
```

Remove other useless columns (subjectively). See column reference from [BigQuery Export schema](https://support.google.com/analytics/answer/3437719)
``` {r rm-other-useless-columns}
rm_cols <- c(
  # isMobile is enough
  'device.deviceCategory',
  # Country, region, city are enough
  'geoNetwork.continent',
  'geoNetwork.subContinent',
  'geoNetwork.metro',
  # Can be substituted by pageViews
  'totals.bounces',
  # Always unique, use fullVisitorId instead
  'sessionId',
  # visitId is also almost always unique
  # However, I still think fullVisitorId is enough
  'visitId',
  # Always unique, creativeId or campaignId would more helpful
  'trafficSource.adwordsClickInfo.gclId',
  # Only false or NA (no "true" value)
  'trafficSource.adwordsClickInfo.isVideoAd'
)

for (col in rm_cols) {
  dfl$train[[col]] <- NULL
  dfl$test[[col]] <- NULL
}

cat('Number of rows:', dim(dfl$train)[1], '\n')

dfl$train %>%
  as.list %>%
  rapply(na.omit, how = 'replace') %>%
  str(give.attr = FALSE)
```

Fix data types (from character)
``` {r fix-data-types}
for (df in names(dfl)) {
  # # Keep as character because min, max, etc is not useful for id
  # dfl[[df]]$fullVisitorId <- as.integer(dfl[[df]]$fullVisitorId)

  # newVisits should actually be a boolean (logical)
  # However, "1" (character) when converted to logical will result to NA
  # While "1" to integer will result to 1 (as it should be)
  dfl[[df]]$totals.newVisits <- as.integer(dfl[[df]]$totals.newVisits)

  dfl[[df]]$totals.hits <- as.integer(dfl[[df]]$totals.hits)
  dfl[[df]]$totals.pageviews <- as.integer(dfl[[df]]$totals.pageviews)
  dfl[[df]]$trafficSource.adwordsClickInfo.page <- as.integer(dfl[[df]]$trafficSource.adwordsClickInfo.page)

  if (df != 'test') {
    # The transactionRevenue column doesn't exist on test data
    # We use numeric (double) in case of integer overflow
    dfl[[df]]$totals.transactionRevenue <- as.numeric(dfl[[df]]$totals.transactionRevenue)
    # Divide by 1,000,000 (as real USD)
    # https://www.kaggle.com/c/ga-customer-revenue-prediction/discussion/65775
    dfl[[df]]$totals.transactionRevenue <- dfl[[df]]$totals.transactionRevenue / 1e+06
  }
}
```

Check column consistenty between train and test data
``` {r check-col-consistency}
# If only the transactionRevenue is missing, then you're good
setdiff(colnames(dfl$train), colnames(dfl$test))
```

# Data Visualization

Plot daily visits and revenue
``` {r plot-daily}
#| message: false
#| fig-height: 8

df <- dfl$train %>%
  group_by(date) %>%
  summarise(
    count = n(),
    revenue = sum(totals.transactionRevenue, na.rm = TRUE),
    newVisits = sum(totals.newVisits, na.rm = TRUE)
  )

g1 <- df %>%
  ggplot(aes(x = date, y = newVisits)) +
  geom_line(color = 'steelblue') +
  geom_smooth(color = 'orange') +
  scale_x_date(date_breaks = 'months', date_labels = '%b') +
  scale_y_continuous(n.breaks = 10) +
  labs(title = 'Daily New Visits', x = 'date (2016-2017)', y = 'new visitors')

g2 <- df %>%
  ggplot(aes(x = date, y = count)) +
  geom_line(color = 'steelblue') +
  geom_smooth(color = 'orange') +
  scale_x_date(date_breaks = 'months', date_labels = '%b') +
  scale_y_continuous(n.breaks = 10) +
  labs(title = 'Daily Visits (All)', x = 'date (2016-2017)', y = 'visitors')

g3 <- df %>%
  ggplot(aes(x = date, y = revenue)) +
  geom_line(color = 'steelblue') +
  geom_smooth(color = 'orange') +
  scale_x_date(date_breaks = 'months', date_labels = '%b') +
  scale_y_continuous(n.breaks = 10) +
  labs(title = 'Daily Revenue', x = 'date (2016-2017)', y = 'revenue ($)')

g1 / g2 / g3
```

Plot hourly visits and revenue
``` {r plot-hourly}
#| message: false

df <- dfl$train %>%
  mutate(visitHour = hour(visitStartTime)) %>%
  group_by(visitHour) %>%
  summarise(
    count = n(),
    revenue = sum(totals.transactionRevenue, na.rm = TRUE)
  )

g1 <- df %>%
  ggplot(aes(x = visitHour, y = count)) +
  geom_line(color = 'steelblue') +
  geom_smooth(color = 'orange') +
  scale_x_continuous(breaks = seq(0, 23, by = 1)) +
  scale_y_continuous(n.breaks = 10) +
  labs(title = 'Aggregate Hourly Visits', x = 'hour (0-23)', y = 'sum visitors')

g2 <- df %>%
  ggplot(aes(x = visitHour, y = revenue)) +
  geom_line(color = 'steelblue') +
  geom_smooth(color = 'orange') +
  scale_x_continuous(breaks = seq(0, 23, by = 1)) +
  scale_y_continuous(n.breaks = 10, limits = c(0, 1250)) +
  labs(title = 'Aggregate Hourly Revenue', x = 'hour (0-23)', y = 'sum revenue ($)')

g1 / g2
```

Standardize country names before plotting map
``` {r standardize-country-names}
df <- dfl$train %>%
  filter(!is.na(geoNetwork.country)) %>%
  group_by(geoNetwork.country) %>%
  summarise(
    count = n(),
    revenue = sum(totals.transactionRevenue, na.rm = TRUE)
  ) %>%
  arrange(-revenue)

# Contains long, lat, region (country name), etc
world_map <- map_data('world')

# Check which country names that don't exist on the map
# # setdiff(df$geoNetwork.country, world_map$region)

# Standardize the country names
# "nomatch = NULL" will keep the original name on error
df$iso3 <- countrycode(
  df$geoNetwork.country, origin = 'country.name',
  destination = 'iso.name.en', nomatch = NULL
)
world_map$region <- countrycode(
  world_map$region, origin = 'country.name',
  destination = 'iso.name.en', nomatch = NULL
)

# Check again after the standardization
# # setdiff(df$iso3, world_map$region)

# The map doesn't have Hong Kong so we need list it as China
df <- df %>% mutate(iso3 = ifelse(iso3 == 'Hong Kong', 'China', iso3))
```

Plot visits and revenue by country
``` {r plot-map}
#| warning: false

cut_nice <- function(var, br) {
  cut(
    # Discretize (bin) continuos "var" based on "breaks"
    var, breaks = br,
    # Will give a normal looking, nice formatted label
    # The default one is too scientific (uses "e" notation, etc)
    labels = paste0(br[1:length(br) - 1] + 1, '-', br[2:length(br)])
  )
}

g1 <- df %>%
  # "map_id" on the dataframe should match the map id/region column
  # https://github.com/tidyverse/ggplot2/issues/504
  ggplot(aes(map_id = iso3)) +
  geom_map(aes(fill = cut_nice(count, seq(0, 5000, 150))), map = world_map) +
  geom_polygon(
    data = world_map, aes(x = long, y = lat, group = group, map_id = region),
    fill = NA, color = 'gray50', linewidth = 0.2
  ) + 
  # Expand the coordinates range (show the whole world)
  expand_limits(x = world_map$long, y = world_map$lat) +
  # Fix the map aspect ratio
  coord_fixed() +
  # Use "brewer" if discrete or "distiller" if continuous value
  # The color intensity depends on the "fill" argument on "aes"
  scale_fill_brewer(
    name = 'visitors', palette = 'RdYlGn',
    direction = 1
  ) +
  dark_theme_void()

g2 <- df %>%
  ggplot(aes(map_id = iso3)) +
  geom_map(aes(fill = cut_nice(revenue, seq(0, 15000, 100))), map = world_map) +
  geom_polygon(
    data = world_map, aes(x = long, y = lat, group = group, map_id = region),
    fill = NA, color = 'gray50', linewidth = 0.2
  ) + 
  expand_limits(x = world_map$long, y = world_map$lat) +
  coord_fixed() +
  scale_fill_brewer(
    name = 'revenue ($)', palette = 'RdYlGn',
    direction = 1
  ) +
  dark_theme_void()

g1 / g2 + plot_annotation(title = 'Total Visits and Revenue by Country')
```

``` {r chart-helper-function}
#| echo: false

# Get member count and percentage of top-N group on a column
# The column can be browser type, country, etc
group_count <- function(df, col, n_group) {
  # Convert variable name to string
  # Use !!sym(str) to convert back to variable
  str <- deparse(substitute(col))

  df %>%
    filter(!is.na(!!sym(str))) %>%
    # Use ".by" so we don't need to use "group_by" before
    summarise(count = n(), .by = !!sym(str)) %>%
    ungroup %>%
    arrange(-count) %>%
    mutate(
      # Will rank the value (e.g. Chrome, Safari) based on count
      # dense_rank() is similar but allows duplicate ranking
      rank = row_number(),
      # Take only the N-highest ranked group (the rest as "Others")
      !!str := ifelse(rank <= n_group, !!sym(str), 'Others')
    ) %>%
    # Group again to recalculate the sum count of "Others" group
    summarise(count = sum(count), .by = !!sym(str)) %>%
    mutate(pct = percent(count / sum(count)))
}

# Similar as above but to sum the revenue (not count member)
group_revenue <- function(df, col, n_group) {
  str <- deparse(substitute(col))

  df %>%
    summarise(
      revenue = sum(totals.transactionRevenue, na.rm = TRUE),
      .by = !!sym(str)
    ) %>%
    ungroup %>%
    arrange(-revenue) %>%
    mutate(
      rank = row_number(),
      !!str := ifelse(rank <= n_group, !!sym(str), 'Others')
    ) %>%
    summarise(revenue = sum(revenue), .by = !!sym(str)) %>%
    mutate(pct = percent(revenue / sum(revenue)))
}
```

Plot visits and revenue by browser type

Some references:

- [why not pie chart?](https://r-graph-gallery.com/piechart-ggplot2.html)
- [bar chart starter](https://r-graphics.org/recipe-bar-graph-labels)
- [remove legend from chart](https://stackoverflow.com/questions/35618260/remove-legend-ggplot-2-2)
- [remove X axis entirely](https://stackoverflow.com/questions/35090883/remove-all-of-x-axis-labels-in-ggplot)

``` {r plot-browser}
g1 <- dfl$train %>%
  # Get the count and percentage (pct) of each browser
  group_count(device.browser, n_group = 5) %>%
  ggplot(aes(
    x = reorder(device.browser, count),
    y = count,
    fill = device.browser
  )) +
  # Make a horizontal bar chart without color legend
  geom_col(show.legend = FALSE) +
  coord_flip() +
  # Show percentage and increase the Y axis limit in case of overflow
  geom_text(aes(label = pct), hjust = -0.1) + ylim(c(0, 7000)) + 
  labs(title = 'Most Used Browser by Visitors', x = 'browser')

g2 <- dfl$train %>%
  group_revenue(device.browser, n_group = 5) %>%
  ggplot(aes(
    x = reorder(device.browser, revenue),
    y = revenue,
    fill = device.browser
  )) +
  geom_col(show.legend = FALSE) +
  coord_flip() +
  geom_text(aes(label = pct), hjust = -0.1) + ylim(c(0, 11000)) + 
  labs(title = 'Total Revenue by Browser', x = 'browser', y = 'revenue ($)')

g1 / g2
```

Plot visits and revenue by OS type
``` {r plot-system}
g1 <- dfl$train %>%
  group_count(device.operatingSystem, n_group = 5) %>%
  ggplot(aes(
    x = reorder(device.operatingSystem, count),
    y = count,
    fill = device.operatingSystem
  )) +
  geom_col(show.legend = FALSE) +
  coord_flip() +
  geom_text(aes(label = pct), hjust = -0.1) + ylim(c(0, 4000)) + 
  labs(title = 'Most Used OS by Visitors', x = 'operating system')

g2 <- dfl$train %>%
  group_revenue(device.operatingSystem, n_group = 5) %>%
  ggplot(aes(
    x = reorder(device.operatingSystem, revenue),
    y = revenue,
    fill = device.operatingSystem
  )) +
  geom_col(show.legend = FALSE) +
  coord_flip() +
  geom_text(aes(label = pct), hjust = -0.1) + ylim(c(0, 8000)) + 
  labs(title = 'Total Revenue by OS', x = 'operating system')

g1 / g2
```

Plot visits and revenue by channel
``` {r plot-channel}
g1 <- dfl$train %>%
  # trafficSource.medium is also similar but has more NA values
  group_count(channelGrouping, n_group = n_distinct(.$channelGrouping)) %>%
  ggplot(aes(
    x = reorder(channelGrouping, count),
    y = count,
    fill = channelGrouping
  )) +
  geom_col(show.legend = FALSE) +
  coord_flip() +
  geom_text(aes(label = pct), hjust = -0.1) + ylim(c(0, 4200)) + 
  labs(title = 'Traffic Source by Channel ("channelGrouping")', x = 'channel')

g2 <- dfl$train %>%
  group_revenue(channelGrouping, n_group = n_distinct(.$channelGrouping)) %>%
  ggplot(aes(
    x = reorder(channelGrouping, revenue),
    y = revenue,
    fill = channelGrouping
  )) +
  geom_col(show.legend = FALSE) +
  coord_flip() +
  geom_text(aes(label = pct), hjust = -0.1) + ylim(c(0, 6500)) +
  labs(title = 'Total Revenue by Channel', x = 'channel', y = 'revenue ($)')

g1 / g2
```

Plot visits and revenue by referrer
``` {r plot-referrer}
g1 <- dfl$train %>%
  group_count(trafficSource.source, n_group = 8) %>%
  ggplot(aes(
    x = reorder(trafficSource.source, count),
    y = count,
    fill = trafficSource.source
  )) +
  geom_col(show.legend = FALSE) +
  coord_flip() +
  geom_text(aes(label = pct), hjust = -0.1, size = 3) + ylim(c(0, 4500)) + 
  labs(title = 'Traffic Source by Referrer ("utm_source")', x = 'channel')

g2 <- dfl$train %>%
  group_revenue(trafficSource.source, n_group = 8) %>%
  ggplot(aes(
    x = reorder(trafficSource.source, revenue),
    y = revenue,
    fill = trafficSource.source
  )) +
  geom_col(show.legend = FALSE) +
  coord_flip() +
  geom_text(aes(label = pct), hjust = -0.1, size = 3) + ylim(c(0, 6500)) + 
  labs(title = 'Total Revenue by Referrer', x = 'channel', y = 'revenue ($)')

g1 / g2
```

Plot visits and revenue by keywords
``` {r plot-keywords}
#| warning: false
#| cache: true

df <- dfl$train %>%
  filter(!is.na(trafficSource.keyword)) %>%
  mutate(
    trafficSource.keyword = str_remove_all(trafficSource.keyword, '\\+'),
    trafficSource.keyword = str_remove_all(
      # Remove http(s), www, and trailing slash
      # To workaround "gridtext" converting url to HTML tag
      trafficSource.keyword, '(?:https?:\\/\\/)?(?:www\\.)?(?:\\/$)?'
    ),
  )

g1 <- df %>%
  summarise(count = n(), .by = trafficSource.keyword) %>%
  ggplot(aes(label = trafficSource.keyword, size = count, color = count)) +
  geom_text_wordcloud_area(rm_outside = TRUE) +
  scale_size_area(max_size = 40) +
  scale_color_gradient(low = 'yellow4', high = 'yellow') +
  labs(title = 'Most Used Keywords (Search Engine and "utm_term")') +
  dark_theme_void()

g2 <- df %>%
  group_revenue(trafficSource.keyword, n_group = 4) %>%
  ggplot(aes(
    x = reorder(trafficSource.keyword, revenue),
    y = revenue,
    fill = trafficSource.keyword
  )) +
  geom_col(show.legend = FALSE) +
  coord_flip() +
  geom_text(aes(label = pct), hjust = -0.1, size = 3) + ylim(c(0, 2000)) + 
  labs(title = 'Total Revenue by Keywords', x = 'keywords', y = 'revenue ($)')

# "free" will remove the word cloud X-axis repositioning
# By default it will be aligned with the bar chart X-axis
free(g1) / g2 +
# Custom height proportion for the plots
plot_layout(heights = c(3.5, 1.5))
```

Plot revenue correlation with hits and page views

References:

- [Google Analytics hits](https://www.digishuffle.com/blogs/google-analytics-hits/)
- [what are page views?](https://www.reliablesoft.net/what-are-pageviews-in-google-analytics/)

In summary:

- Visits: The number of visits (not unique per person, use `newVisits` instead)
- Hits: The numbers of interaction triggered (can be a page load, clicking item, purchase, etc; depends on how it's configured)
- Page views: A type of hit that is triggered on page load, refresh, and change

``` {r revenue-per-hits}
#| warning: false
#| fig-height: 8

g1 <- dfl$train %>%
  summarise(
    revenue = median(totals.transactionRevenue, na.rm = TRUE),
    .by = totals.hits
  ) %>%
  ggplot(aes(x = totals.hits, y = revenue)) +
  geom_line(color = 'steelblue') +
  geom_point(color = 'steelblue', size = 0.5) +
  geom_smooth(color = 'orange') +
  scale_x_continuous(breaks = seq(0, 160, by = 10)) +
  scale_y_continuous(n.breaks = 10, limits = c(0, 550)) +
  labs(
    title = 'Aggregate Revenue by Hits',
    x = 'hits', y = 'median revenue ($)'
  )

g2 <- dfl$train %>%
  summarise(
    revenue = median(totals.transactionRevenue, na.rm = TRUE),
    .by = totals.pageviews
  ) %>%
  ggplot(aes(x = totals.pageviews, y = revenue)) +
  geom_line(color = 'steelblue') +
  geom_point(color = 'steelblue', size = 0.5) +
  geom_smooth(color = 'orange') +
  scale_x_continuous(breaks = seq(0, 110, by = 10)) +
  scale_y_continuous(n.breaks = 10, limits = c(0, 600)) +
  labs(
    title = 'Aggregate Revenue by Page Views',
    x = 'page views', y = 'median revenue ($)'
  )

g3 <- dfl$train %>%
  summarise(
    hits = median(totals.hits, na.rm = TRUE),
    .by = totals.pageviews
  ) %>%
  ggplot(aes(x = totals.pageviews, y = hits)) +
  geom_line(color = 'steelblue') +
  geom_point(color = 'steelblue', size = 0.5) +
  geom_smooth(color = 'orange') +
  scale_x_continuous(breaks = seq(0, 110, by = 10)) +
  scale_y_continuous(n.breaks = 10, limits = c(0, 175)) +
  labs(
    title = 'Aggregate Hits by Page Views',
    x = 'page views', y = 'median hits'
  )

g1 / g2 / g3
```

# Notes

For my future self:

- `.` and `.data` are different, be careful when using them on pipes (`%>%`)
- `sapply` may change the original column class type (e.g. from date to double)
- When ranking something on a dataframe, `row_number` or `dense_rank` is usually what you need. `order`, `sort`, and `rank` can produce a weird result
- Knitting can be a lot slower than using Jupyter kernel, unless we `cache` the chunks. However, cache can be outdated and the detection mechanism sucks, so we may occasionally need to delete the cache manually

# Todo

- Make a forecast (e.g. using `fable` or `tidymodels`). Some reference that might be interesting:
  - https://nicholasrjenkins.science/post/tidy_time_series/tts_r/
  - https://rc2e.com/timeseriesanalysis
- Learn to take more accurate sampling (see [reference](https://www.kaggle.com/code/captcalculator/a-very-extensive-gstore-exploratory-analysis) for chart comparison)
- Make test set from `train.csv` since we are using stratified sampling anyway. Also make sure train and test dataframe doesn't collide (contains same rows)
- Alluvial/sankey diagram (see [reference](https://www.kaggle.com/code/kailex/r-eda-for-gstore-glm-keras-xgb))